package org.example;

import java.util.HashMap;
import java.util.Map;
import java.util.Properties;
import java.util.Timer;
import java.util.TimerTask;

import com.inditex.ofda.epmlqsrc.model.autogenerated.measurement.v2.Measurement;
import com.inditex.ofda.epmlqsrc.model.autogenerated.measurement.v2.WindowTime;

import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.common.utils.Bytes;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.Topology;
import org.apache.kafka.streams.kstream.Consumed;
import org.apache.kafka.streams.kstream.Grouped;
import org.apache.kafka.streams.kstream.KTable;
import org.apache.kafka.streams.kstream.Materialized;
import org.apache.kafka.streams.state.KeyValueStore;

public class Application {

  private static final String KAFKA_INPUT_TOPIC = "comercial-dev-za.coreanalyticsQaMeasurements.kafkaProbe.v2";

  private static final String PTR_INPUT_TOPIC = "comercial-dev-za.coreanalyticsQaMeasurements.databaseProbe.v2";

  private static final Map<String, Object> kafkaConfig = Map.of(
      "bootstrap.servers", "localhost:9092",
      "schema.registry.url", " http://localhost:8081",
      "auto.offset.reset", "earliest",
      "processing.guarantee", "exactly_once",
      "acks", "all",
      "application.id", "KafkaPtrComparator",
      "client.id", "KafkaPtrComparator",
      "group.id", "KafkaPtrComparator"
  );

  private static final CustomSerdes SERDES = new CustomSerdes((String) kafkaConfig.get("schema.registry.url"));

  private static final Map<String, MetricValue> metrics = new HashMap<>();

  public static void main(String[] args) {
    System.out.println("Started app...");
    Properties properties = new Properties();
    properties.putAll(kafkaConfig);
    KafkaStreams kafkaStreams = new KafkaStreams(getTopology(), properties);
    System.out.println("Created KafkaStreams");
    kafkaStreams.start();
    System.out.println("Started KafkaStreams");
    Runtime.getRuntime().addShutdownHook(new Thread(kafkaStreams::close));
    printMetrics();
  }

  private static void printMetrics() {
    new Timer().schedule(new TimerTask() {
      @Override
      public void run() {
        metrics.forEach((key, value) -> value.print());
        printMetrics();
      }
    }, 30000);
  }

  public static Topology getTopology() {
    StreamsBuilder streamsBuilder = new StreamsBuilder();

    KTable<String, Measurement> kafkaTable =
        streamsBuilder.stream(KAFKA_INPUT_TOPIC, Consumed.with(Serdes.String(), SERDES.getMeasurementSerde()))
            .peek((k, v) -> System.out.println("Kafka Measurement: " + v))
            .selectKey((k, v) -> getKey(v))
            .groupByKey(Grouped.with("reduceKafka", Serdes.String(), SERDES.getMeasurementSerde()))
            .reduce((a, b) -> b, Materialized.<String, Measurement, KeyValueStore<Bytes, byte[]>>as("reduceKafka")
                .withKeySerde(Serdes.String())
                .withValueSerde(SERDES.getMeasurementSerde()));

    KTable<String, Measurement> ptrTable =
        streamsBuilder.stream(PTR_INPUT_TOPIC, Consumed.with(Serdes.String(), SERDES.getMeasurementSerde()))
            .peek((k, v) -> System.out.println("Database Measurement: " + v))
            .selectKey((k, v) -> getKey(v))
            .groupByKey(Grouped.with("reduceDatabase", Serdes.String(), SERDES.getMeasurementSerde()))
            .reduce((a, b) -> b, Materialized.<String, Measurement, KeyValueStore<Bytes, byte[]>>as("reduceDatabase")
                .withKeySerde(Serdes.String())
                .withValueSerde(SERDES.getMeasurementSerde()));

    KTable<String, String> join = kafkaTable.join(ptrTable, Application::getMetric,
        Materialized.<String, String, KeyValueStore<Bytes, byte[]>>as("joinTable")
            .withKeySerde(Serdes.String())
            .withValueSerde(Serdes.String()));

    join.toStream().foreach(Application::updateAndPrintMap);

    return streamsBuilder.build();
  }

  private static void updateAndPrintMap(String key, String value) {
    String[] valueSplited = value.split("->");

    MetricValue metricValue = MetricValue.builder()
        .pipeline(valueSplited[0])
        .partition(valueSplited[1])
        .windowStartTimestamp(valueSplited[2])
        .kafkaValue(Integer.parseInt(valueSplited[3]))
        .ptrValue(Integer.parseInt(valueSplited[4]))
        .inSync(Boolean.parseBoolean(valueSplited[5]))
        .build();

    metrics.put(key, metricValue);
  }

  private static String getMetric(Measurement leftValue, Measurement rightValue) {
    return rightValue.getIngestionPipeline() + "->" +
        leftValue.getMeasure().getPartition() + "->" +
        ((WindowTime) leftValue.getMeasure().getTimeFrame()).getFrom() + "->" +
        leftValue.getMeasure().getValue() + "->" +
        rightValue.getMeasure().getValue() + "->" +
        leftValue.getMeasure().getValue().equals(rightValue.getMeasure().getValue());
  }

  private static String getKey(Measurement measurement) {
    return measurement.getIngestionPipeline() + "-" +
        measurement.getName() + "-" +
        measurement.getPartitionName() + "-" +
        measurement.getType() + "-" +
        measurement.getMeasure().getPartition() + "-" +
        ((WindowTime) measurement.getMeasure().getTimeFrame()).getFrom() + "-" +
        ((WindowTime) measurement.getMeasure().getTimeFrame()).getTo();
  }

}
